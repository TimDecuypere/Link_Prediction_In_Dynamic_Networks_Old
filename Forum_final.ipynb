{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node2vec\n",
    "---\n",
    "[node2vec](http://snap.stanford.edu/node2vec/) for link prediction:\n",
    "1. Perform train-test split\n",
    "1. Train skip-gram model on random walks within training graph\n",
    "2. Get node embeddings from skip-gram model\n",
    "3. Create bootstrapped edge embeddings by taking the Hadamard product of node embeddings\n",
    "4. Train a logistic regression classifier on these edge embeddings (possible edge --> edge score between 0-1)\n",
    "5. Evaluate these edge embeddings on the validation and test edge sets\n",
    "\n",
    "node2vec source code: https://github.com/aditya-grover/node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Dynamic Enron employees\n",
    "Number of nodes: 151\n",
    "Number of edges: 1612\n",
    "Number of time frames: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EGO_USER = 0 # which ego network to look at\n",
    "\n",
    "# Load pickled (adj, feat) tuple\n",
    "#network_dir = './fb-processed/{0}-adj-feat.pkl'.format(EGO_USER)\n",
    "#with open(network_dir, 'rb') as f:\n",
    "#    adj, features = pickle.load(f)\n",
    "    \n",
    "#g = nx.Graph(adj) # re-create graph using node indices (0 to num_nodes-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load in the networks, make the training and test edge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "7046\n"
     ]
    }
   ],
   "source": [
    "MasterGraph = nx.read_edgelist(\"m_forum.csv\", nodetype=int, delimiter=\",\")\n",
    "for edge in MasterGraph.edges():\n",
    "    MasterGraph[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "print MasterGraph.number_of_nodes()\n",
    "print MasterGraph.number_of_edges()\n",
    "\n",
    "G1 = nx.read_edgelist(\"m1_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G1.edges():\n",
    "    G1[edge[0]][edge[1]]['weight'] = 1\n",
    "G2 = nx.read_edgelist(\"m2_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G2.edges():\n",
    "    G2[edge[0]][edge[1]]['weight'] = 1\n",
    "G3 = nx.read_edgelist(\"m3_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G3.edges():\n",
    "    G3[edge[0]][edge[1]]['weight'] = 1\n",
    "G4 = nx.read_edgelist(\"m4_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G4.edges():\n",
    "    G4[edge[0]][edge[1]]['weight'] = 1\n",
    "G13 = nx.read_edgelist(\"m13_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G13.edges():\n",
    "    G13[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "## All the nodes are in MasterNodes    \n",
    "MasterNodes = MasterGraph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training - Test split  \n",
    "'''''\n",
    "first add all the nodes that are in MasterGraph but not in \n",
    "G4\n",
    "'''''\n",
    "for i in MasterNodes:\n",
    "    if i not in G4.nodes():\n",
    "        G4.add_node(i)\n",
    "        \n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gae.preprocessing import mask_test_edges\n",
    "np.random.seed(0) # make sure train-test split is consistent between notebooks\n",
    "\n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G4)\n",
    "\n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.0, prevent_disconnect = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 899\n",
      "Total edges: 2202\n",
      "Training edges (positive): 1541\n",
      "Training edges (negative): 1541\n",
      "Validation edges (positive): 0\n",
      "Validation edges (negative): 0\n",
      "Test edges (positive): 660\n",
      "Test edges (negative): 660\n"
     ]
    }
   ],
   "source": [
    "# Inspect train/test split\n",
    "print \"Total nodes:\", adj_sparse.shape[0]\n",
    "print \"Total edges:\", int(adj_sparse.nnz/2) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
    "print \"Training edges (positive):\", len(train_edges)\n",
    "print \"Training edges (negative):\", len(train_edges_false)\n",
    "print \"Validation edges (positive):\", len(val_edges)\n",
    "print \"Validation edges (negative):\", len(val_edges_false)\n",
    "print \"Test edges (positive):\", len(test_edges)\n",
    "print \"Test edges (negative):\", len(test_edges_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive training edges are in the edgelist \"train_edges\".\n",
    "\n",
    "The negative training edges are in the edgelist \"train_edges_false\".\n",
    "\n",
    "The positive test edges are in the edgelist \"test_edges\".\n",
    "\n",
    "The negative test edges are in the edgelist \"test_edges_false\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: add all the nodes in G1, G2, G3 that are not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add all the nodes that are in the MasterGraph but not in \n",
    "G1, G2 and G3\n",
    "'''\n",
    "for i in MasterNodes:\n",
    "    if i not in G1.nodes():\n",
    "        G1.add_node(i)\n",
    "    if i not in G2.nodes():\n",
    "        G2.add_node(i)\n",
    "    if i not in G3.nodes():\n",
    "        G3.add_node(i)\n",
    "    if i not in G13.nodes():\n",
    "        G13.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges before removal: \n",
      "G1:   3249\n",
      "G2:   3088\n",
      "G3:   3032\n",
      "G13:   6299\n",
      "Edges after removal: \n",
      "G1:   3237\n",
      "G2:   3078\n",
      "G3:   3021\n",
      "G13:   6282\n"
     ]
    }
   ],
   "source": [
    "print \"Edges before removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()\n",
    "print \"G13:  \", G13.number_of_edges()\n",
    "\n",
    "\n",
    "'''\n",
    "for every snapshot, delete all the edges that occur in the \n",
    "test set, this is important because the training of node2vec\n",
    "can only be done on the training network and not on edges that\n",
    "are used for testing\n",
    "'''\n",
    "for i in range(0,len(test_edges)):\n",
    "        if G1.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G1.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G2.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G2.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G3.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G3.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G13.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G13.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "            \n",
    "print \"Edges after removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()\n",
    "print \"G13:  \", G13.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train node2vec (Learn Node Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sam\\anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import node2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec settings\n",
    "# NOTE: When p = q = 1, this is equivalent to DeepWalk\n",
    "\n",
    "P = 1 # Return hyperparameter\n",
    "Q = 1 # In-out hyperparameter\n",
    "WINDOW_SIZE = 10 # Context size for optimization\n",
    "NUM_WALKS = 10 # Number of walks per source\n",
    "WALK_LENGTH = 80 # Length of walk per source\n",
    "DIMENSIONS = 200 # Embedding dimension\n",
    "DIRECTED = False # Graph directed/undirected\n",
    "WORKERS = 8 # Num. parallel workers\n",
    "ITER = 1 # SGD epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, generate walks\n",
    "G1_n2v = node2vec.Graph(G1, DIRECTED, P, Q) # create node2vec graph instance\n",
    "G2_n2v = node2vec.Graph(G2, DIRECTED, P, Q)\n",
    "G3_n2v = node2vec.Graph(G3, DIRECTED, P, Q)\n",
    "G13_n2v = node2vec.Graph(G13, DIRECTED, P, Q)\n",
    "\n",
    "G1_n2v.preprocess_transition_probs()\n",
    "G2_n2v.preprocess_transition_probs()\n",
    "G3_n2v.preprocess_transition_probs()\n",
    "G13_n2v.preprocess_transition_probs()\n",
    "\n",
    "walksG1 = G1_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "walksG2 = G2_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "walksG3 = G3_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "walksG13 = G13_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "\n",
    "walksG1 = [map(str, walk) for walk in walksG1]\n",
    "walksG2 = [map(str, walk) for walk in walksG2]\n",
    "walksG3 = [map(str, walk) for walk in walksG3]\n",
    "walksG13 = [map(str, walk) for walk in walksG13]\n",
    "\n",
    "# Train skip-gram model\n",
    "modelG1 = Word2Vec(walksG1, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG2 = Word2Vec(walksG2, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG3 = Word2Vec(walksG3, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG13 = Word2Vec(walksG13, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "\n",
    "\n",
    "# Store embeddings mapping\n",
    "emb_mappingsG1 = modelG1.wv\n",
    "emb_mappingsG2 = modelG2.wv\n",
    "emb_mappingsG3 = modelG3.wv\n",
    "emb_mappingsG13 = modelG13.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Edge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node embeddings matrix (rows = nodes, columns = embedding features)\n",
    "emb_listG1 = []\n",
    "emb_listG2 = []\n",
    "emb_listG3 = []\n",
    "emb_listG13 = []\n",
    "\n",
    "for node_index in range(1, adj_sparse.shape[0]+1):\n",
    "    node_str = str(node_index)\n",
    "    \n",
    "    node_embG1 = emb_mappingsG1[node_str]\n",
    "    node_embG2 = emb_mappingsG2[node_str]\n",
    "    node_embG3 = emb_mappingsG3[node_str]\n",
    "    node_embG13 = emb_mappingsG13[node_str]\n",
    "    \n",
    "    emb_listG1.append(node_embG1)\n",
    "    emb_listG2.append(node_embG2)\n",
    "    emb_listG3.append(node_embG3)\n",
    "    emb_listG13.append(node_embG13)\n",
    "    \n",
    "emb_matrixG1 = np.vstack(emb_listG1)\n",
    "emb_matrixG2 = np.vstack(emb_listG2)\n",
    "emb_matrixG3 = np.vstack(emb_listG3)\n",
    "emb_matrixG13 = np.vstack(emb_listG13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bootstrapped edge embeddings (as is done in node2vec paper)\n",
    "    # Edge embedding for (v1, v2) = hadamard product of node embeddings for v1, v2\n",
    "def get_edge_embeddings_dynamic(edge_list):\n",
    "    embs = []\n",
    "    for edge in edge_list:\n",
    "        \n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        \n",
    "        embG1_1 = emb_matrixG1[node1]\n",
    "        embG1_2 = emb_matrixG1[node2]\n",
    "        \n",
    "        embG2_1 = emb_matrixG2[node1]\n",
    "        embG2_2 = emb_matrixG2[node2]\n",
    "        \n",
    "        embG3_1 = emb_matrixG3[node1]\n",
    "        embG3_2 = emb_matrixG3[node2]\n",
    "        \n",
    "        edge_embG1 = np.multiply(embG1_1, embG1_2)\n",
    "        edge_embG2 = np.multiply(embG2_1, embG2_2)\n",
    "        edge_embG3 = np.multiply(embG3_1, embG3_2)\n",
    "        \n",
    "        edge_emb = np.hstack((edge_embG1,edge_embG2, edge_embG3))\n",
    "        embs.append(edge_emb)\n",
    "        \n",
    "    embs = np.array(embs)\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_embeddings_static(edge_list):\n",
    "    embs_s = []\n",
    "    for edge in edge_list:\n",
    "        \n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        \n",
    "        embG13_1 = emb_matrixG13[node1]\n",
    "        embG13_2 = emb_matrixG13[node2]\n",
    "        \n",
    "        edge_embG13 = np.multiply(embG13_1, embG13_2)\n",
    "        embs_s.append(edge_embG13)\n",
    "        \n",
    "    embs_s = np.array(embs_s)\n",
    "    \n",
    "    return embs_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DYNAMIC\n",
    "# Train-set edge embeddings\n",
    "pos_train_edge_embs_d = get_edge_embeddings_dynamic(train_edges)\n",
    "neg_train_edge_embs_d = get_edge_embeddings_dynamic(train_edges_false)\n",
    "train_edge_embs_d = np.concatenate([pos_train_edge_embs_d, neg_train_edge_embs_d])\n",
    "\n",
    "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
    "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
    "\n",
    "# Val-set edge embeddings, labels\n",
    "pos_val_edge_embs_d = get_edge_embeddings_dynamic(val_edges)\n",
    "neg_val_edge_embs_d = get_edge_embeddings_dynamic(val_edges_false)\n",
    "val_edge_embs_d = np.concatenate([pos_val_edge_embs_d, neg_val_edge_embs_d])\n",
    "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
    "\n",
    "# Test-set edge embeddings, labels\n",
    "pos_test_edge_embs_d = get_edge_embeddings_dynamic(test_edges)\n",
    "neg_test_edge_embs_d = get_edge_embeddings_dynamic(test_edges_false)\n",
    "test_edge_embs_d = np.concatenate([pos_test_edge_embs_d, neg_test_edge_embs_d])\n",
    "\n",
    "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
    "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STATIC\n",
    "# Train-set edge embeddings\n",
    "pos_train_edge_embs_s = get_edge_embeddings_static(train_edges)\n",
    "neg_train_edge_embs_s = get_edge_embeddings_static(train_edges_false)\n",
    "train_edge_embs_s = np.concatenate([pos_train_edge_embs_s, neg_train_edge_embs_s])\n",
    "\n",
    "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
    "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
    "\n",
    "# Val-set edge embeddings, labels\n",
    "pos_val_edge_embs_s = get_edge_embeddings_static(val_edges)\n",
    "neg_val_edge_embs_s = get_edge_embeddings_static(val_edges_false)\n",
    "val_edge_embs_s = np.concatenate([pos_val_edge_embs_s, neg_val_edge_embs_s])\n",
    "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
    "\n",
    "# Test-set edge embeddings, labels\n",
    "pos_test_edge_embs_s = get_edge_embeddings_static(test_edges)\n",
    "neg_test_edge_embs_s = get_edge_embeddings_static(test_edges_false)\n",
    "test_edge_embs_s = np.concatenate([pos_test_edge_embs_s, neg_test_edge_embs_s])\n",
    "\n",
    "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
    "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Edge Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the basic topological classifiers are calculated for the test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier on train-set edge embeddings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "edge_classifier_lr_stat = LogisticRegression(random_state=0)\n",
    "edge_classifier_lr_stat.fit(train_edge_embs_s, train_edge_labels)\n",
    "\n",
    "edge_classifier_lr_dyn = LogisticRegression(random_state=0)\n",
    "edge_classifier_lr_dyn.fit(train_edge_embs_d, train_edge_labels)\n",
    "\n",
    "edge_classifier_RF_stat = RandomForestClassifier(n_estimators = 50)\n",
    "edge_classifier_RF_stat.fit(train_edge_embs_s, train_edge_labels)\n",
    "\n",
    "edge_classifier_RF_dyn = RandomForestClassifier(n_estimators = 50)\n",
    "edge_classifier_RF_dyn.fit(train_edge_embs_d, train_edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "# val_preds = edge_classifier.predict_proba(val_edge_embs)[:, 1]\n",
    "# val_roc = roc_auc_score(val_edge_labels, val_preds)\n",
    "# val_ap = average_precision_score(val_edge_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "test_preds_lr_s = edge_classifier_lr_stat.predict_proba(test_edge_embs_s)[:, 1]\n",
    "test_roc_lr_s = roc_auc_score(test_edge_labels, test_preds_lr_s)\n",
    "test_ap_lr_s = average_precision_score(test_edge_labels, test_preds_lr_s)\n",
    "\n",
    "test_preds_lr_d = edge_classifier_lr_dyn.predict_proba(test_edge_embs_d)[:, 1]\n",
    "test_roc_lr_d = roc_auc_score(test_edge_labels, test_preds_lr_d)\n",
    "test_ap_lr_d = average_precision_score(test_edge_labels, test_preds_lr_d)\n",
    "\n",
    "test_preds_rf_s = edge_classifier_RF_stat.predict_proba(test_edge_embs_s)[:, 1]\n",
    "test_roc_rf_s = roc_auc_score(test_edge_labels, test_preds_rf_s)\n",
    "test_ap_rf_s = average_precision_score(test_edge_labels, test_preds_rf_s)\n",
    "\n",
    "test_preds_rf_d = edge_classifier_RF_dyn.predict_proba(test_edge_embs_d)[:, 1]\n",
    "test_roc_rf_d = roc_auc_score(test_edge_labels, test_preds_rf_d)\n",
    "test_ap_rf_d = average_precision_score(test_edge_labels, test_preds_rf_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec Test ROC score logistic regression static:  0.765119375573921\n",
      "node2vec Test ROC score logistic regression dynamic:  0.8276331496786042\n",
      "node2vec Test ROC score random forest static:  0.8079832415059688\n",
      "node2vec Test ROC score random forest dynamic:  0.848593893480257\n",
      "node2vec Test AP score logistic regression static:  0.785494756968407\n",
      "node2vec Test AP score logistic regression dynamic:  0.8467831184469556\n",
      "node2vec Test AP score random forest static:  0.7963042492123757\n",
      "node2vec Test AP score random forest dynamic:  0.8453818853509836\n"
     ]
    }
   ],
   "source": [
    "# print 'node2vec Validation ROC score: ', str(val_roc)\n",
    "# print 'node2vec Validation AP score: ', str(val_ap)\n",
    "print 'node2vec Test ROC score logistic regression static: ', str(test_roc_lr_s)\n",
    "print 'node2vec Test ROC score logistic regression dynamic: ', str(test_roc_lr_d)\n",
    "print 'node2vec Test ROC score random forest static: ', str(test_roc_rf_s)\n",
    "print 'node2vec Test ROC score random forest dynamic: ', str(test_roc_rf_d)\n",
    "print 'node2vec Test AP score logistic regression static: ', str(test_ap_lr_s)\n",
    "print 'node2vec Test AP score logistic regression dynamic: ', str(test_ap_lr_d)\n",
    "print 'node2vec Test AP score random forest static: ', str(test_ap_rf_s)\n",
    "print 'node2vec Test AP score random forest dynamic: ', str(test_ap_rf_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lr_s, recall_lr_s, _ = precision_recall_curve(test_edge_labels, test_preds_lr_s)\n",
    "precision_rf_s, recall_rf_s, _ = precision_recall_curve(test_edge_labels, test_preds_rf_s)\n",
    "precision_lr_d, recall_lr_d, _ = precision_recall_curve(test_edge_labels, test_preds_lr_d)\n",
    "precision_rf_d, recall_rf_d, _ = precision_recall_curve(test_edge_labels, test_preds_rf_d)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.step(recall_lr_s, precision_lr_s, color=\"b\", alpha=1, where='post', label = \"Logistic Regression/Static\")\n",
    "plt.step(recall_lr_d, precision_lr_d, color=\"lime\", alpha=1, where='post', label = \"Logistic Regression/Dynamic\")\n",
    "plt.step(recall_rf_s, precision_rf_s, color=\"salmon\", alpha=1, where='post', label = \"Random Forest/Static\")\n",
    "plt.step(recall_rf_d, precision_rf_d, color=\"olive\", alpha=1, where='post', label = \"Random Forest/Dynamic\")\n",
    "#plt.fill_between(recall_lr_s, precision_lr_s, step=\"post\", alpha=0.2, color=\"b\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve Enron employees')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "fig.savefig(\"AUPR_forum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
