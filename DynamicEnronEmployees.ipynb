{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node2vec\n",
    "---\n",
    "[node2vec](http://snap.stanford.edu/node2vec/) for link prediction:\n",
    "1. Perform train-test split\n",
    "1. Train skip-gram model on random walks within training graph\n",
    "2. Get node embeddings from skip-gram model\n",
    "3. Create bootstrapped edge embeddings by taking the Hadamard product of node embeddings\n",
    "4. Train a logistic regression classifier on these edge embeddings (possible edge --> edge score between 0-1)\n",
    "5. Evaluate these edge embeddings on the validation and test edge sets\n",
    "\n",
    "node2vec source code: https://github.com/aditya-grover/node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Dynamic Enron employees\n",
    "Number of nodes: 151\n",
    "Number of edges: 1612\n",
    "Number of time frames: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EGO_USER = 0 # which ego network to look at\n",
    "\n",
    "# Load pickled (adj, feat) tuple\n",
    "#network_dir = './fb-processed/{0}-adj-feat.pkl'.format(EGO_USER)\n",
    "#with open(network_dir,continuous = False 'rb') as f:\n",
    "#    adj, features = pickle.load(f)\n",
    "    \n",
    "#g = nx.Graph(adj) # re-create graph using node indices (0 to num_nodes-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Continuous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = True\n",
    "original = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load in the networks, make the training and test edge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "1588\n"
     ]
    }
   ],
   "source": [
    "MasterGraph = nx.read_edgelist(\"Enron-employees/m_enron_employees.csv\", nodetype=int, delimiter=\",\")\n",
    "for edge in MasterGraph.edges():\n",
    "    MasterGraph[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "print MasterGraph.number_of_nodes()\n",
    "print MasterGraph.number_of_edges()\n",
    "\n",
    "G1 = nx.read_edgelist(\"Enron-employees/m_enron_employees_1.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G1.edges():\n",
    "    G1[edge[0]][edge[1]]['weight'] = 1\n",
    "G2 = nx.read_edgelist(\"Enron-employees/m_enron_employees_2.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G2.edges():\n",
    "    G2[edge[0]][edge[1]]['weight'] = 1\n",
    "G3 = nx.read_edgelist(\"Enron-employees/m_enron_employees_3.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G3.edges():\n",
    "    G3[edge[0]][edge[1]]['weight'] = 1\n",
    "G4 = nx.read_edgelist(\"Enron-employees/m_enron_employees_4.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G4.edges():\n",
    "    G4[edge[0]][edge[1]]['weight'] = 1\n",
    "G5 = nx.read_edgelist(\"Enron-employees/m_enron_employees_5.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G5.edges():\n",
    "    G5[edge[0]][edge[1]]['weight'] = 1\n",
    "G6 = nx.read_edgelist(\"Enron-employees/m_enron_employees_6.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G6.edges():\n",
    "    G6[edge[0]][edge[1]]['weight'] = 1\n",
    "G7 = nx.read_edgelist(\"Enron-employees/m_enron_employees_7.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G7.edges():\n",
    "    G7[edge[0]][edge[1]]['weight'] = 1\n",
    "G8 = nx.read_edgelist(\"Enron-employees/m_enron_employees_8.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G8.edges():\n",
    "    G8[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "#G17 = nx.read_edgelist(\"Enron-employees/m_enron_employees_17.csv\", nodetype = int, delimiter = \",\")\n",
    "#for edge in G17.edges():\n",
    "#    G17[edge[0]][edge[1]]['weight'] = 1\n",
    "#\n",
    "    \n",
    "    \n",
    "## All the nodes are in MasterNodes    \n",
    "MasterNodes = MasterGraph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "with open('Enron-employees/m_enron_employees_17_weighted.csv', 'rb') as f:\n",
    "    weighted_edges_17 = [[int(x) for x in rec] for rec in csv.reader(f, delimiter=',')]\n",
    "    weighted_edges_17 = map(tuple, weighted_edges_17)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #reader = csv.reader(f)\n",
    "    #weighted_edges_17 = map(tuple, reader)\n",
    "    \n",
    "\n",
    "#print weighted_edges_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G17 = nx.MultiGraph()\n",
    "#G17 = G17.read_weighted_edgelist('Enron-employees/m_enron_employees_17_weighted.csv', delimiter=\",\", nodetype=int)\n",
    "G17.add_weighted_edges_from(weighted_edges_17)\n",
    "\n",
    "\n",
    "#for edge in G.edges():\n",
    "#    print edge\n",
    "#    #print G17[edge[0]][edge[1]][0]['weight']  #[edge[2]]\n",
    "#    print G17[edge[0]][edge[1]]\n",
    "#    #print len(G17[edge[0]][edge[1]])\n",
    "#help(nx.read_edgelist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G17[1][2]) \n",
    "G17[1][2][0]['weight']\n",
    "weight_index = []\n",
    "weight_index.append(random.randint(0,len(G[cur][i])-1))\n",
    "\n",
    "print cur, i\n",
    "print len(G[cur][i])\n",
    "print len(G[cur][i])-1\n",
    "print G[cur][i][weight_index[-1]]['weight']\n",
    "print weight_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in G17.edges(): #2,137,73,49\n",
    "    print edge #1,97,130,73,..\n",
    "\n",
    "cur = 6\n",
    "list_neighbors =G17.neighbors(cur)\n",
    "\n",
    "#for p in list_neighbors: print p\n",
    "cur=6\n",
    "\n",
    "\n",
    "print len(G17[6][8])\n",
    "print G17[6][8][0]['weight']\n",
    "    \n",
    "for i in list_neighbors:\n",
    "    for j in range(0,len(G17[cur][i])):\n",
    "        print G17[cur][i][j]['weight'] \n",
    "        print i\n",
    "        print j\n",
    "        print 'gezien'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print G1[2][137][0]['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=G17\n",
    "walk = [1,1]\n",
    "cur_nbrs=[]\n",
    "cur = walk[-1]\n",
    "\n",
    "\n",
    "list_neighbors = G.neighbors(cur)\n",
    "if len(walk) > 1:\n",
    "    for i in list_neighbors:\n",
    "        for j in range(0,len(G[cur][i])):\n",
    "            if G[cur][i][j]['weight'] > 2: #G.edges[walk[-2]][cur][0]:\n",
    "                print G[cur][i][j]\n",
    "                cur_nbrs.append(i)                     \n",
    "print cur_nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    '''\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    '''\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    '''\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    '''\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cur_nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node = 1\n",
    "cont = True\n",
    "walk_length = 5\n",
    "\n",
    "alias_nodes = self.alias_nodes\n",
    "alias_edges = self.alias_edges\n",
    "\n",
    "\n",
    "\n",
    "walk = [start_node]\n",
    "weight_index=[]\n",
    "wur_nbrs=[]\n",
    "while len(walk) < walk_length:\n",
    "           cur = walk[-1]\n",
    "           \n",
    "           if cont == True:\n",
    "               list_neighbors = G.neighbors(cur)\n",
    "               if len(walk) > 1:\n",
    "                   for i in list_neighbors:\n",
    "                       for j in range(0,len(G[cur][i])):\n",
    "                           if G[cur][i][j]['weight'] > G[walk[-2]][cur][weight_index[-1]]['weight']:\n",
    "                               cur_nbrs.append(i)                     \n",
    "           else: \n",
    "                       cur_nbrs = sorted(G.neighbors(cur))\n",
    "                       \n",
    "           if len(cur_nbrs) > 0:\n",
    "               if len(walk) == 1:\n",
    "                   walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "               else:\n",
    "                   prev = walk[-2]\n",
    "                   next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
    "                       alias_edges[(prev, cur)][1])]\n",
    "                   walk.append(next)\n",
    "                   weight_index.append(random.randint(0,len(G[cur][next])-1))\n",
    "           else:\n",
    "               break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training - Test split  \n",
    "'''''\n",
    "first add all the nodes that are in MasterGraph but not in \n",
    "G8\n",
    "'''''\n",
    "for i in MasterNodes:\n",
    "    if i not in G8.nodes():\n",
    "        G8.add_node(i)\n",
    "        \n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gae.preprocessing import mask_test_edges\n",
    "np.random.seed(0) # make sure train-test split is consistent between notebooks\n",
    "\n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G8)\n",
    "\n",
    "adj_train_del_del, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.0, prevent_disconnect = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 151\n",
      "Total edges: 758\n",
      "Training edges (positive): 516\n",
      "Training edges (negative): 516\n",
      "Validation edges (positive): 0\n",
      "Validation edges (negative): 0\n",
      "Test edges (positive): 220\n",
      "Test edges (negative): 220\n"
     ]
    }
   ],
   "source": [
    "# Inspect train/test split\n",
    "print \"Total nodes:\", adj_sparse.shape[0]\n",
    "print \"Total edges:\", int(adj_sparse.nnz/2) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
    "print \"Training edges (positive):\", len(train_edges)\n",
    "print \"Training edges (negative):\", len(train_edges_false)\n",
    "print \"Validation edges (positive):\", len(val_edges)\n",
    "print \"Validation edges (negative):\", len(val_edges_false)\n",
    "print \"Test edges (positive):\", len(test_edges)\n",
    "print \"Test edges (negative):\", len(test_edges_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive training edges are in the edgelist \"train_edges\".\n",
    "\n",
    "The negative training edges are in the edgelist \"train_edges_false\".\n",
    "\n",
    "The positive test edges are in the edgelist \"test_edges\".\n",
    "\n",
    "The negative test edges are in the edgelist \"test_edges_false\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: add all the nodes in G1, G2, G3 that are not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add all the nodes that are in the MasterGraph but not in \n",
    "G1, G2 and G3\n",
    "'''\n",
    "for i in MasterNodes:\n",
    "    if i not in G1.nodes():\n",
    "        G1.add_node(i)\n",
    "    if i not in G2.nodes():\n",
    "        G2.add_node(i)\n",
    "    if i not in G3.nodes():\n",
    "        G3.add_node(i)\n",
    "    if i not in G4.nodes():\n",
    "        G4.add_node(i)\n",
    "    if i not in G5.nodes():\n",
    "        G5.add_node(i)\n",
    "    if i not in G6.nodes():\n",
    "        G6.add_node(i)\n",
    "    if i not in G7.nodes():\n",
    "        G7.add_node(i)\n",
    "    if i not in G17.nodes():\n",
    "        G17.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges before removal: \n",
      "G1:   101\n",
      "G2:   149\n",
      "G3:   270\n",
      "G4:   348\n",
      "G5:   400\n",
      "G6:   540\n",
      "G7:   816\n",
      "G17:   34314\n",
      "Edges after removal: \n",
      "G1:   98\n",
      "G2:   147\n",
      "G3:   265\n",
      "G4:   341\n",
      "G5:   389\n",
      "G6:   525\n",
      "G7:   791\n",
      "G17:   34280\n"
     ]
    }
   ],
   "source": [
    "print \"Edges before removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()\n",
    "print \"G4:  \", G4.number_of_edges()\n",
    "print \"G5:  \", G5.number_of_edges()\n",
    "print \"G6:  \", G6.number_of_edges()\n",
    "print \"G7:  \", G7.number_of_edges()\n",
    "print \"G17:  \", G17.number_of_edges()\n",
    "\n",
    "\n",
    "'''\n",
    "for every snapshot, delete all the edges that occur in the \n",
    "test set, this is important because the training of node2vec\n",
    "can only be done on the training network and not on edges that\n",
    "are used for testing\n",
    "'''\n",
    "for i in range(0,len(test_edges)):\n",
    "        if G1.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G1.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G2.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G2.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G3.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G3.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G4.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G4.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G5.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G5.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G6.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G6.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G7.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G7.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G17.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G17.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "            \n",
    "print \"Edges after removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()\n",
    "print \"G4:  \", G4.number_of_edges()\n",
    "print \"G5:  \", G5.number_of_edges()\n",
    "print \"G6:  \", G6.number_of_edges()\n",
    "print \"G7:  \", G7.number_of_edges()\n",
    "print \"G17:  \", G17.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train node2vec (Learn Node Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim_Dell_XPS\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "if original == False:\n",
    "    import node2vecWeight_bidirectional as node2vec\n",
    "else:\n",
    "    import node2vec\n",
    "    \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec settings\n",
    "# NOTE: When p = q = 1, this is equivalent to DeepWalk\n",
    "\n",
    "P = 1 # Return hyperparameter\n",
    "Q = 1 # In-out hyperparameter\n",
    "WINDOW_SIZE = 10 # Context size for optimization\n",
    "NUM_WALKS = 10 # Number of walks per source\n",
    "WALK_LENGTH = 80 # Length of walk per source\n",
    "DIMENSIONS = 128 # Embedding dimension\n",
    "DIRECTED = False # Graph directed/undirected\n",
    "WORKERS = 8 # Num. parallel workers\n",
    "ITER = 1 # SGD epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, generate walks\n",
    "G1_n2v = node2vec.Graph(G1, DIRECTED, P, Q) # create node2vec graph instance\n",
    "G2_n2v = node2vec.Graph(G2, DIRECTED, P, Q)\n",
    "G3_n2v = node2vec.Graph(G3, DIRECTED, P, Q)\n",
    "G4_n2v = node2vec.Graph(G4, DIRECTED, P, Q) \n",
    "G5_n2v = node2vec.Graph(G5, DIRECTED, P, Q)\n",
    "G6_n2v = node2vec.Graph(G6, DIRECTED, P, Q)\n",
    "G7_n2v = node2vec.Graph(G7, DIRECTED, P, Q)\n",
    "G17_n2v = node2vec.Graph(G17, DIRECTED, P, Q)\n",
    "\n",
    "G1_n2v.preprocess_transition_probs()\n",
    "G2_n2v.preprocess_transition_probs()\n",
    "G3_n2v.preprocess_transition_probs()\n",
    "G4_n2v.preprocess_transition_probs()\n",
    "G5_n2v.preprocess_transition_probs()\n",
    "G6_n2v.preprocess_transition_probs()\n",
    "G7_n2v.preprocess_transition_probs()\n",
    "G17_n2v.preprocess_transition_probs()\n",
    "\n",
    "\n",
    "if original == False:    \n",
    "    walksG1 = G1_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG2 = G2_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG3 = G3_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG4 = G4_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG5 = G5_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG6 = G6_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG7 = G7_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "    walksG17 = G17_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH, continuous)\n",
    "else:\n",
    "\n",
    "    walksG1 = G1_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG2 = G2_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG3 = G3_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG4 = G4_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG5 = G5_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG6 = G6_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG7 = G7_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "    walksG17 = G17_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "\n",
    "walksG1 = [map(str, walk) for walk in walksG1]\n",
    "walksG2 = [map(str, walk) for walk in walksG2]\n",
    "walksG3 = [map(str, walk) for walk in walksG3]\n",
    "walksG4 = [map(str, walk) for walk in walksG4]\n",
    "walksG5 = [map(str, walk) for walk in walksG5]\n",
    "walksG6 = [map(str, walk) for walk in walksG6]\n",
    "walksG7 = [map(str, walk) for walk in walksG7]\n",
    "walksG17 = [map(str, walk) for walk in walksG17]\n",
    "\n",
    "# Train skip-gram model\n",
    "modelG1 = Word2Vec(walksG1, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG2 = Word2Vec(walksG2, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG3 = Word2Vec(walksG3, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG4 = Word2Vec(walksG4, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG5 = Word2Vec(walksG5, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG6 = Word2Vec(walksG6, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG7 = Word2Vec(walksG7, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG17 = Word2Vec(walksG17, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "\n",
    "\n",
    "# Store embeddings mapping\n",
    "emb_mappingsG1 = modelG1.wv\n",
    "emb_mappingsG2 = modelG2.wv\n",
    "emb_mappingsG3 = modelG3.wv\n",
    "emb_mappingsG4 = modelG4.wv\n",
    "emb_mappingsG5 = modelG5.wv\n",
    "emb_mappingsG6 = modelG6.wv\n",
    "emb_mappingsG7 = modelG7.wv\n",
    "emb_mappingsG17 = modelG17.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Edge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G17 = nx.Graph(G17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node embeddings matrix (rows = nodes, columns = embedding features)\n",
    "emb_listG1 = []\n",
    "emb_listG2 = []\n",
    "emb_listG3 = []\n",
    "emb_listG4 = []\n",
    "emb_listG5 = []\n",
    "emb_listG6 = []\n",
    "emb_listG7 = []\n",
    "emb_listG17 = []\n",
    "\n",
    "for node_index in range(1, adj_sparse.shape[0]+1):\n",
    "    node_str = str(node_index)\n",
    "    \n",
    "    node_embG1 = emb_mappingsG1[node_str]\n",
    "    node_embG2 = emb_mappingsG2[node_str]\n",
    "    node_embG3 = emb_mappingsG3[node_str]\n",
    "    node_embG4 = emb_mappingsG4[node_str]\n",
    "    node_embG5 = emb_mappingsG5[node_str]\n",
    "    node_embG6 = emb_mappingsG6[node_str]\n",
    "    node_embG7 = emb_mappingsG7[node_str]\n",
    "    node_embG17 = emb_mappingsG17[node_str]\n",
    "    \n",
    "    emb_listG1.append(node_embG1)\n",
    "    emb_listG2.append(node_embG2)\n",
    "    emb_listG3.append(node_embG3)\n",
    "    emb_listG4.append(node_embG4)\n",
    "    emb_listG5.append(node_embG5)\n",
    "    emb_listG6.append(node_embG6)\n",
    "    emb_listG7.append(node_embG7)\n",
    "    emb_listG17.append(node_embG17)\n",
    "    \n",
    "emb_matrixG1 = np.vstack(emb_listG1)\n",
    "emb_matrixG2 = np.vstack(emb_listG2)\n",
    "emb_matrixG3 = np.vstack(emb_listG3)\n",
    "emb_matrixG4 = np.vstack(emb_listG4)\n",
    "emb_matrixG5 = np.vstack(emb_listG5)\n",
    "emb_matrixG6 = np.vstack(emb_listG6)\n",
    "emb_matrixG7 = np.vstack(emb_listG7)\n",
    "emb_matrixG17 = np.vstack(emb_listG17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bootstrapped edge embeddings (as is done in node2vec paper)\n",
    "    # Edge embedding for (v1, v2) = hadamard product of node embeddings for v1, v2\n",
    "def get_edge_embeddings_dynamic(edge_list):\n",
    "    embs = []\n",
    "    for edge in edge_list:\n",
    "        \n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        \n",
    "        embG1_1 = emb_matrixG1[node1]\n",
    "        embG1_2 = emb_matrixG1[node2]\n",
    "        \n",
    "        embG2_1 = emb_matrixG2[node1]\n",
    "        embG2_2 = emb_matrixG2[node2]\n",
    "        \n",
    "        embG3_1 = emb_matrixG3[node1]\n",
    "        embG3_2 = emb_matrixG3[node2]\n",
    "        \n",
    "        embG4_1 = emb_matrixG4[node1]\n",
    "        embG4_2 = emb_matrixG4[node2]\n",
    "        \n",
    "        embG5_1 = emb_matrixG5[node1]\n",
    "        embG5_2 = emb_matrixG5[node2]\n",
    "        \n",
    "        embG6_1 = emb_matrixG6[node1]\n",
    "        embG6_2 = emb_matrixG6[node2]\n",
    "        \n",
    "        embG7_1 = emb_matrixG7[node1]\n",
    "        embG7_2 = emb_matrixG7[node2]\n",
    "        \n",
    "        edge_embG1 = np.multiply(embG1_1, embG1_2)\n",
    "        edge_embG2 = np.multiply(embG2_1, embG2_2)\n",
    "        edge_embG3 = np.multiply(embG3_1, embG3_2)\n",
    "        edge_embG4 = np.multiply(embG4_1, embG4_2)\n",
    "        edge_embG5 = np.multiply(embG5_1, embG5_2)\n",
    "        edge_embG6 = np.multiply(embG6_1, embG6_2)\n",
    "        edge_embG7 = np.multiply(embG7_1, embG7_2)\n",
    "        \n",
    "        edge_emb = np.hstack((edge_embG1,edge_embG2, edge_embG3, edge_embG4, edge_embG5, edge_embG6, edge_embG7))\n",
    "        embs.append(edge_emb)\n",
    "        \n",
    "    embs = np.array(embs)\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_embeddings_static(edge_list):\n",
    "    embs_s = []\n",
    "    for edge in edge_list:\n",
    "        \n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        \n",
    "        embG17_1 = emb_matrixG17[node1]\n",
    "        embG17_2 = emb_matrixG17[node2]\n",
    "        \n",
    "        edge_embG17 = np.multiply(embG17_1, embG17_2)\n",
    "        embs_s.append(edge_embG17)\n",
    "        \n",
    "    embs_s = np.array(embs_s)\n",
    "    \n",
    "    return embs_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DYNAMIC\n",
    "# Train-set edge embeddings\n",
    "pos_train_edge_embs_d = get_edge_embeddings_dynamic(train_edges)\n",
    "neg_train_edge_embs_d = get_edge_embeddings_dynamic(train_edges_false)\n",
    "train_edge_embs_d = np.concatenate([pos_train_edge_embs_d, neg_train_edge_embs_d])\n",
    "\n",
    "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
    "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
    "\n",
    "# Val-set edge embeddings, labels\n",
    "pos_val_edge_embs_d = get_edge_embeddings_dynamic(val_edges)\n",
    "neg_val_edge_embs_d = get_edge_embeddings_dynamic(val_edges_false)\n",
    "val_edge_embs_d = np.concatenate([pos_val_edge_embs_d, neg_val_edge_embs_d])\n",
    "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
    "\n",
    "# Test-set edge embeddings, labels\n",
    "pos_test_edge_embs_d = get_edge_embeddings_dynamic(test_edges)\n",
    "neg_test_edge_embs_d = get_edge_embeddings_dynamic(test_edges_false)\n",
    "test_edge_embs_d = np.concatenate([pos_test_edge_embs_d, neg_test_edge_embs_d])\n",
    "\n",
    "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
    "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STATIC\n",
    "# Train-set edge embeddings\n",
    "pos_train_edge_embs_s = get_edge_embeddings_static(train_edges)\n",
    "neg_train_edge_embs_s = get_edge_embeddings_static(train_edges_false)\n",
    "train_edge_embs_s = np.concatenate([pos_train_edge_embs_s, neg_train_edge_embs_s])\n",
    "\n",
    "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
    "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
    "\n",
    "# Val-set edge embeddings, labels\n",
    "pos_val_edge_embs_s = get_edge_embeddings_static(val_edges)\n",
    "neg_val_edge_embs_s = get_edge_embeddings_static(val_edges_false)\n",
    "val_edge_embs_s = np.concatenate([pos_val_edge_embs_s, neg_val_edge_embs_s])\n",
    "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
    "\n",
    "# Test-set edge embeddings, labels\n",
    "pos_test_edge_embs_s = get_edge_embeddings_static(test_edges)\n",
    "neg_test_edge_embs_s = get_edge_embeddings_static(test_edges_false)\n",
    "test_edge_embs_s = np.concatenate([pos_test_edge_embs_s, neg_test_edge_embs_s])\n",
    "\n",
    "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
    "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Edge Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the basic topological classifiers are calculated for the test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier on train-set edge embeddings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "edge_classifier_lr_stat = LogisticRegression(random_state=0)\n",
    "edge_classifier_lr_stat.fit(train_edge_embs_s, train_edge_labels)\n",
    "\n",
    "edge_classifier_lr_dyn = LogisticRegression(random_state=0)\n",
    "edge_classifier_lr_dyn.fit(train_edge_embs_d, train_edge_labels)\n",
    "\n",
    "edge_classifier_RF_stat = RandomForestClassifier(n_estimators = 50)\n",
    "edge_classifier_RF_stat.fit(train_edge_embs_s, train_edge_labels)\n",
    "\n",
    "edge_classifier_RF_dyn = RandomForestClassifier(n_estimators = 50)\n",
    "edge_classifier_RF_dyn.fit(train_edge_embs_d, train_edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Gradient Boosted Regression Trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "edge_classifier_gb_stat = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, max_depth=8, random_state=0).fit(train_edge_embs_s, train_edge_labels)\n",
    "edge_classifier_gb_dyn = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, max_depth=8, random_state=0).fit(train_edge_embs_d, train_edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "# val_preds = edge_classifier.predict_proba(val_edge_embs)[:, 1]\n",
    "# val_roc = roc_auc_score(val_edge_labels, val_preds)\n",
    "# val_ap = average_precision_score(val_edge_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "test_preds_lr_s = edge_classifier_lr_stat.predict_proba(test_edge_embs_s)[:, 1]\n",
    "test_roc_lr_s = roc_auc_score(test_edge_labels, test_preds_lr_s)\n",
    "test_ap_lr_s = average_precision_score(test_edge_labels, test_preds_lr_s)\n",
    "\n",
    "test_preds_lr_d = edge_classifier_lr_dyn.predict_proba(test_edge_embs_d)[:, 1]\n",
    "test_roc_lr_d = roc_auc_score(test_edge_labels, test_preds_lr_d)\n",
    "test_ap_lr_d = average_precision_score(test_edge_labels, test_preds_lr_d)\n",
    "\n",
    "test_preds_rf_s = edge_classifier_RF_stat.predict_proba(test_edge_embs_s)[:, 1]\n",
    "test_roc_rf_s = roc_auc_score(test_edge_labels, test_preds_rf_s)\n",
    "test_ap_rf_s = average_precision_score(test_edge_labels, test_preds_rf_s)\n",
    "\n",
    "test_preds_rf_d = edge_classifier_RF_dyn.predict_proba(test_edge_embs_d)[:, 1]\n",
    "test_roc_rf_d = roc_auc_score(test_edge_labels, test_preds_rf_d)\n",
    "test_ap_rf_d = average_precision_score(test_edge_labels, test_preds_rf_d)\n",
    "\n",
    "test_preds_gb_s = edge_classifier_gb_stat.predict_proba(test_edge_embs_s)[:, 1]\n",
    "test_roc_gb_s = roc_auc_score(test_edge_labels, test_preds_gb_s)\n",
    "test_ap_gb_s = average_precision_score(test_edge_labels, test_preds_gb_s)\n",
    "\n",
    "test_preds_gb_d = edge_classifier_gb_dyn.predict_proba(test_edge_embs_d)[:, 1]\n",
    "test_roc_gb_d = roc_auc_score(test_edge_labels, test_preds_gb_d)\n",
    "test_ap_gb_d = average_precision_score(test_edge_labels, test_preds_gb_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec Test ROC score logistic regression static:  0.5092148760330578\n",
      "node2vec Test ROC score logistic regression dynamic:  0.5092148760330578\n",
      "node2vec Test ROC score random forest static:  0.6232334710743801\n",
      "node2vec Test ROC score random forest dynamic:  0.6601652892561984\n",
      "node2vec Test ROC score gradient boosting static:  0.6130165289256198\n",
      "node2vec Test ROC score gradient boosting dynamic:  0.6268181818181817\n",
      "node2vec Test AP score logistic regression static:  0.5150212830391636\n",
      "node2vec Test AP score logistic regression dynamic:  0.5150212830391636\n",
      "node2vec Test AP score random forest static:  0.6119218661609744\n",
      "node2vec Test AP score random forest dynamic:  0.6344603553587846\n",
      "node2vec Test AP score gradient boosting static:  0.6308989082298229\n",
      "node2vec Test AP score gradient boosting dynamic:  0.645228941565654\n"
     ]
    }
   ],
   "source": [
    "# print 'node2vec Validation ROC score: ', str(val_roc)\n",
    "# print 'node2vec Validation AP score: ', str(val_ap)\n",
    "print 'node2vec Test ROC score logistic regression static: ', str(test_roc_lr_s)\n",
    "print 'node2vec Test ROC score logistic regression dynamic: ', str(test_roc_lr_d)\n",
    "print 'node2vec Test ROC score random forest static: ', str(test_roc_rf_s)\n",
    "print 'node2vec Test ROC score random forest dynamic: ', str(test_roc_rf_d)\n",
    "print 'node2vec Test ROC score gradient boosting static: ', str(test_roc_gb_s)\n",
    "print 'node2vec Test ROC score gradient boosting dynamic: ', str(test_roc_gb_d)\n",
    "print 'node2vec Test AP score logistic regression static: ', str(test_ap_lr_s)\n",
    "print 'node2vec Test AP score logistic regression dynamic: ', str(test_ap_lr_d)\n",
    "print 'node2vec Test AP score random forest static: ', str(test_ap_rf_s)\n",
    "print 'node2vec Test AP score random forest dynamic: ', str(test_ap_rf_d)\n",
    "print 'node2vec Test AP score gradient boosting static: ', str(test_ap_gb_s)\n",
    "print 'node2vec Test AP score gradient boosting dynamic: ', str(test_ap_gb_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC curve\n",
    "fpr_lr_s, tpr_lr_s, _ = roc_curve(test_edge_labels, test_preds_lr_s)\n",
    "fpr_lr_d, tpr_lr_d, _ = roc_curve(test_edge_labels, test_preds_lr_d)\n",
    "fpr_rf_s, tpr_rf_s, _ = roc_curve(test_edge_labels, test_preds_rf_s)\n",
    "fpr_rf_d, tpr_rf_d, _ = roc_curve(test_edge_labels, test_preds_rf_d)\n",
    "fpr_gb_s, tpr_gb_s, _ = roc_curve(test_edge_labels, test_preds_gb_s)\n",
    "fpr_gb_d, tpr_gb_d, _ = roc_curve(test_edge_labels, test_preds_gb_d)\n",
    "\n",
    "\n",
    "fig_roc = plt.figure()\n",
    "\n",
    "plt.plot([0,1], [0, 1], 'k--')\n",
    "plt.step(fpr_lr_s, tpr_lr_s, color = \"b\", alpha = 1, where = 'post', label = \"Logistic Regression/Static\")\n",
    "plt.step(fpr_lr_d, tpr_lr_d, color = \"lime\", alpha = 1, where = 'post', label = \"Logistic Regression/Dynamic\")\n",
    "plt.step(fpr_rf_s, tpr_rf_s, color = \"salmon\", alpha = 1, where = 'post', label = \"Random Forest/Static\")\n",
    "plt.step(fpr_rf_d, tpr_rf_d, color = \"olive\", alpha = 1, where = 'post', label = \"Random Forest/Dynamic\")\n",
    "plt.step(fpr_gb_s, tpr_gb_s, color = \"red\", alpha = 1, where = 'post', label = \"Gradient Boosting/Static\")\n",
    "plt.step(fpr_gb_d, tpr_gb_d, color = \"grey\", alpha = 1, where = 'post', label = \"Gradient Boosting/Dynamic\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('ROC curve Enron employees')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "fig_roc.savefig(\"ROC_Enron.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision - Recall curve\n",
    "\n",
    "precision_lr_s, recall_lr_s, _ = precision_recall_curve(test_edge_labels, test_preds_lr_s)\n",
    "precision_rf_s, recall_rf_s, _ = precision_recall_curve(test_edge_labels, test_preds_rf_s)\n",
    "precision_lr_d, recall_lr_d, _ = precision_recall_curve(test_edge_labels, test_preds_lr_d)\n",
    "precision_rf_d, recall_rf_d, _ = precision_recall_curve(test_edge_labels, test_preds_rf_d)\n",
    "precision_gb_s, recall_gb_s, _ = precision_recall_curve(test_edge_labels, test_preds_gb_s)\n",
    "precision_gb_d, recall_gb_d, _ = precision_recall_curve(test_edge_labels, test_preds_gb_d)\n",
    "\n",
    "fig_aupr = plt.figure()\n",
    "\n",
    "plt.step(recall_lr_s, precision_lr_s, color=\"b\", alpha=1, where='post', label = \"Logistic Regression/Static\")\n",
    "plt.step(recall_lr_d, precision_lr_d, color=\"lime\", alpha=1, where='post', label = \"Logistic Regression/Dynamic\")\n",
    "\n",
    "plt.step(recall_rf_s, precision_rf_s, color=\"salmon\", alpha=1, where='post', label = \"Random Forest/Static\")\n",
    "plt.step(recall_rf_d, precision_rf_d, color=\"olive\", alpha=1, where='post', label = \"Random Forest/Dynamic\")\n",
    "\n",
    "plt.step(recall_gb_s,precision_gb_s,  color=\"red\", alpha=1, where='post', label = \"Gradient Boosting/Static\")\n",
    "plt.step( recall_gb_d,precision_gb_d, color=\"grey\", alpha=1, where='post', label = \"Gradient Boosting/Dynamic\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve Enron employees')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "fig_aupr.savefig(\"AUPR_Enron.png\")\n",
    "\n",
    "#plt.step(recall_rf_s, precision_rf_s, color=\"salmon\", alpha=1, where='post', label = \"Random Forest/Static\")\n",
    "#plt.step(recall_rf_d, precision_rf_d, color=\"olive\", alpha=1, where='post', label = \"Random Forest/Dynamic\")\n",
    "#plt.fill_between(recall_lr_s, precision_lr_s, step=\"post\", alpha=0.2, color=\"b\")\n",
    "\n",
    "#plt.xlabel('Recall')\n",
    "#plt.ylabel('Precision')\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlim([0.0, 1.0])\n",
    "#plt.title('Precision-Recall curve Enron employees')\n",
    "\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "#fig.savefig(\"AUPR_Enron.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline classifiers (Adamic Adar, Jaccard Index & Preferred Attachement)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_score(edges_pos, edges_neg, score_matrix):\n",
    "    # Store positive edge predictions, actual values\n",
    "    preds_pos = []\n",
    "    pos = []\n",
    "    for edge in edges_pos:\n",
    "        preds_pos.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
    "        pos.append(adj_sparse[edge[0], edge[1]]) # actual value (1 for positive)\n",
    "        \n",
    "    # Store negative edge predictions, actual values\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for edge in edges_neg:\n",
    "        preds_neg.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
    "        neg.append(adj_sparse[edge[0], edge[1]]) # actual value (0 for negative)\n",
    "        \n",
    "    # Calculate scores\n",
    "    preds_all = np.hstack([preds_pos, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds_pos)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#adj = nx.adjacency_matrix(MasterGraph)\n",
    "#adj_sparse = nx.to_scipy_sparse_matrix(MasterGraph)    \n",
    "#adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "#    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.00) \n",
    "    \n",
    "adj = nx.adjacency_matrix(G8)\n",
    "    \n",
    "    \n",
    "adj_train = nx.adjacency_matrix(G17)\n",
    "    \n",
    "g_train = nx.from_scipy_sparse_matrix(adj_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Adamic-Adar indexes from g_train\n",
    "aa_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.adamic_adar_index(g_train): # (u, v) = node indices, p = Adamic-Adar index\n",
    "    aa_matrix[u][v] = p\n",
    "    aa_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "aa_matrix = aa_matrix / aa_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "aa_roc, aa_ap = get_roc_score(test_edges, test_edges_false, aa_matrix)\n",
    "\n",
    "print 'Adamic-Adar Test ROC score: ', str(aa_roc)\n",
    "print 'Adamic-Adar Test AP score: ', str(aa_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jaccard Coefficients from g_train\n",
    "jc_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.jaccard_coefficient(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
    "    jc_matrix[u][v] = p\n",
    "    jc_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "jc_matrix = jc_matrix / jc_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "jc_roc, jc_ap = get_roc_score(test_edges, test_edges_false, jc_matrix)\n",
    "\n",
    "print 'Jaccard Coefficient Test ROC score: ', str(jc_roc)\n",
    "print 'Jaccard Coefficient Test AP score: ', str(jc_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preferential Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate, store Adamic-Index scores in array\n",
    "pa_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.preferential_attachment(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
    "    pa_matrix[u][v] = p\n",
    "    pa_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "pa_matrix = pa_matrix / pa_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "pa_roc, pa_ap = get_roc_score(test_edges, test_edges_false, pa_matrix)\n",
    "\n",
    "print 'Preferential Attachment Test ROC score: ', str(pa_roc)\n",
    "print 'Preferential Attachment Test AP score: ', str(pa_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
